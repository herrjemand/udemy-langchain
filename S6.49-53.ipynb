{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The stuff strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\\nThis major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\\n›springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie • Tibshirani • Friedman\\nSecond Edition', metadata={'source': './mixed_data/ESLII_print12_toc (1).pdf', 'page': 0}),\n",
       " Document(page_content='This is page v\\nPrinter: Opaque this\\nTo our parents:\\nValerie and Patrick Hastie\\nVera and Sami Tibshirani\\nFlorence and Harry Friedman\\nand to our families:\\nSamantha, Timothy, and Lynda\\nCharlie, Ryan, Julie, and Cheryl\\nMelanie, Dora, Monika, and Ildiko', metadata={'source': './mixed_data/ESLII_print12_toc (1).pdf', 'page': 1}),\n",
       " Document(page_content='vi', metadata={'source': './mixed_data/ESLII_print12_toc (1).pdf', 'page': 2})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./mixed_data/ESLII_print12_toc (1).pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The stuff chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "index = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "100 3. Linear Methods for Regression\n",
      "\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression funct ion E(Y|X) is\n",
      "linear in the inputs X1,...,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s compu ter era there\n",
      "are still good reasons to study and use them. They are simple a nd often\n",
      "provide an adequate and interpretable description of how th e inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outp erform fancier\n",
      "nonlinear models, especially in situations with small numb ers of training\n",
      "cases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\n",
      "appliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\n",
      "scope. These generalizations are sometimes called basis-f unction methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, w hile in the\n",
      "nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an un derstanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of th e linear methods\n",
      "discussed here.\n",
      "\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "AsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression functio n E(Y|X) is\n",
      "linear, or that the linear model is a reasonable approximati on. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, squar e-root or\n",
      "square;\n",
      "•basisexpansions,suchas X2=X2\n",
      "1,X3=X3\n",
      "1,leadingtoapolynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative input s. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1·X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "methodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonab le if the training\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if theyi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, fo r example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was p roposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalt ies were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron e t al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits th e piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et al.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm simi lar to least\n",
      "angle regression for generalized regression models. Parti al least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage met hods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wide r? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\n",
      "parameteraTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that the relationship between the variables can be represented by a linear equation. The goal of linear regression is to find the best-fitting line that minimizes the difference between the observed data points and the predicted values from the linear equation. This line can then be used to make predictions or understand the relationship between the variables.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\n",
    "    \"What is linear regression?\",\n",
    "    callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Map Reduce strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "100 3. Linear Methods for Regression\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression funct ion E(Y|X) is\n",
      "linear in the inputs X1,...,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s compu ter era there\n",
      "are still good reasons to study and use them. They are simple a nd often\n",
      "provide an adequate and interpretable description of how th e inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outp erform fancier\n",
      "nonlinear models, especially in situations with small numb ers of training\n",
      "cases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\n",
      "appliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\n",
      "scope. These generalizations are sometimes called basis-f unction methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, w hile in the\n",
      "nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an un derstanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of th e linear methods\n",
      "discussed here.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "AsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression functio n E(Y|X) is\n",
      "linear, or that the linear model is a reasonable approximati on. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, squar e-root or\n",
      "square;\n",
      "•basisexpansions,suchas X2=X2\n",
      "1,X3=X3\n",
      "1,leadingtoapolynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative input s. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1·X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "methodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonab le if the training\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if theyi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, fo r example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was p roposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalt ies were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron e t al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits th e piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et al.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm simi lar to least\n",
      "angle regression for generalized regression models. Parti al least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage met hods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wide r? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\n",
      "parameteraTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Given the following extracted parts of a long document and a question, create a final answer. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "______________________\n",
      "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the variables, meaning that the change in the dependent variable is directly proportional to the change in the independent variables. The goal of linear regression is to find the best-fitting line that represents this relationship. The line is determined by estimating the coefficients of the independent variables, which represent the slope and intercept of the line. Linear regression is widely used in various fields for prediction and forecasting purposes.\n",
      "\n",
      "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,...,Xp. Linear models were largely developed in the precomputer age of statistics, but even in today’s computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs affect the output.\n",
      "\n",
      "Linear regression is a statistical modeling technique used to predict a real-valued output based on one or more input variables. It assumes a linear relationship between the input variables and the output variable. The goal is to find the best-fit line or hyperplane that minimizes the sum of the squared differences between the predicted values and the actual values of the output variable. The model is expressed as f(X) = β0 + ∑(Xjβj), where f(X) is the predicted output, β0 and βj are the unknown parameters or coefficients, and Xj represents the input variables.\n",
      "\n",
      "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the variables, and aims to find the best fitting line that represents this relationship. The line is determined by estimating the coefficients (or weights) of the independent variables, which can be used to predict the value of the dependent variable.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the variables and aims to find the best-fitting line or hyperplane that represents this relationship. The model uses coefficients or weights to estimate the value of the dependent variable based on the values of the independent variables. It is widely used for prediction and forecasting purposes in various fields.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type=\"map_reduce\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    \"What is linear regression?\",\n",
    "    callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Refine strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Context information is below.\n",
      "------------\n",
      "100 3. Linear Methods for Regression\n",
      "------------\n",
      "Given the context information and not prior knowledge, answer any questions\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is linear regression?\n",
      "AI: Linear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables, which means that the independent variables have a linear effect on the dependent variable. The goal of linear regression is to find the best-fit line that minimizes the difference between the observed values and the predicted values. This line can then be used to make predictions or understand the relationship between the variables.\n",
      "Human: We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression funct ion E(Y|X) is\n",
      "linear in the inputs X1,...,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s compu ter era there\n",
      "are still good reasons to study and use them. They are simple a nd often\n",
      "provide an adequate and interpretable description of how th e inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outp erform fancier\n",
      "nonlinear models, especially in situations with small numb ers of training\n",
      "cases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\n",
      "appliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\n",
      "scope. These generalizations are sometimes called basis-f unction methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, w hile in the\n",
      "nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an un derstanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of th e linear methods\n",
      "discussed here.\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is linear regression?\n",
      "AI: Linear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables, meaning that the regression function is linear in the inputs. Linear regression models were developed in the precomputer age of statistics but are still widely used today for several reasons. They provide a simple and interpretable description of how the inputs affect the output, and they can sometimes outperform more complex nonlinear models, especially when dealing with limited training data, low signal-to-noise ratio, or sparse data. Additionally, linear methods can be applied to transformations of the inputs, which significantly expands their scope. These generalizations, known as basis-function methods, are discussed in further detail in Chapter 5. Understanding linear methods is considered essential for comprehending nonlinear techniques, as many nonlinear techniques are direct extensions of the linear methods discussed in linear regression.\n",
      "Human: We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "AsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression functio n E(Y|X) is\n",
      "linear, or that the linear model is a reasonable approximati on. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, squar e-root or\n",
      "square;\n",
      "•basisexpansions,suchas X2=X2\n",
      "1,X3=X3\n",
      "1,leadingtoapolynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative input s. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1·X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "methodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonab le if the training\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if theyi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is linear regression?\n",
      "AI: Linear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables, where the regression function is of the form f(X) = β0 + ∑(Xjβj), as described in Equation (3.1). The variables Xj can come from various sources, including quantitative inputs, transformations of quantitative inputs, basis expansions, numeric or \"dummy\" coding of qualitative inputs, and interactions between variables.\n",
      "\n",
      "In linear regression, the goal is to estimate the unknown parameters or coefficients βj based on a set of training data. The most popular estimation method is least squares, where the coefficients are chosen to minimize the residual sum of squares (RSS) between the predicted values and the actual values of the dependent variable. The RSS is calculated using Equation (3.2).\n",
      "\n",
      "It is important to note that the least squares criterion is valid if the training observations represent independent random draws from their population or are conditionally independent given the inputs. Linear regression provides a simple and interpretable description of how the inputs affect the output, and it is widely used in statistics and machine learning for modeling and prediction tasks.\n",
      "Human: We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, fo r example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was p roposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalt ies were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron e t al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits th e piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et al.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm simi lar to least\n",
      "angle regression for generalized regression models. Parti al least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage met hods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wide r? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\n",
      "parameteraTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The given context does not provide any new information related to the definition and explanation of linear regression. Therefore, the original answer remains unchanged.\\n\\nLinear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables and aims to estimate the unknown coefficients based on a set of training data. The most common estimation method is least squares, which minimizes the residual sum of squares between predicted and actual values. Linear regression is widely used for modeling and prediction tasks in statistics and machine learning.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type=\"refine\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    \"What is linear regression?\",\n",
    "    callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Rerank Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapRerankDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "100 3. Linear Methods for Regression\n",
      "---------\n",
      "Question: What is linear regression?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression funct ion E(Y|X) is\n",
      "linear in the inputs X1,...,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s compu ter era there\n",
      "are still good reasons to study and use them. They are simple a nd often\n",
      "provide an adequate and interpretable description of how th e inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outp erform fancier\n",
      "nonlinear models, especially in situations with small numb ers of training\n",
      "cases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\n",
      "appliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\n",
      "scope. These generalizations are sometimes called basis-f unction methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, w hile in the\n",
      "nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an un derstanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of th e linear methods\n",
      "discussed here.\n",
      "---------\n",
      "Question: What is linear regression?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "AsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression functio n E(Y|X) is\n",
      "linear, or that the linear model is a reasonable approximati on. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, squar e-root or\n",
      "square;\n",
      "•basisexpansions,suchas X2=X2\n",
      "1,X3=X3\n",
      "1,leadingtoapolynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative input s. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1·X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "methodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonab le if the training\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if theyi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "---------\n",
      "Question: What is linear regression?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, fo r example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was p roposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalt ies were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron e t al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits th e piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et al.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm simi lar to least\n",
      "angle regression for generalized regression models. Parti al least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage met hods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wide r? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\n",
      "parameteraTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "---------\n",
      "Question: What is linear regression?\n",
      "Helpful Answer:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuriy/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not parse output: Linear regression is a statistical model used to predict a real-valued output based on an input vector. It assumes that the regression function is linear or that the linear model is a reasonable approximation. The model consists of unknown parameters or coefficients (βj) and variables (Xj) that can come from different sources such as quantitative inputs, transformations of quantitative inputs, basis expansions, numeric or \"dummy\" coding of qualitative inputs, and interactions between variables. The parameters (β) are estimated using the least squares method, which minimizes the residual sum of squares.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m      2\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m      3\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mindex\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[1;32m      4\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_rerank\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is linear regression?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mStdOutCallbackHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/base.py:507\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    508\u001b[0m         _output_key\n\u001b[1;32m    509\u001b[0m     ]\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    513\u001b[0m         _output_key\n\u001b[1;32m    514\u001b[0m     ]\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:144\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/base.py:512\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    508\u001b[0m         _output_key\n\u001b[1;32m    509\u001b[0m     ]\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    513\u001b[0m         _output_key\n\u001b[1;32m    514\u001b[0m     ]\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m     )\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:123\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    122\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 123\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/combine_documents/map_rerank.py:168\u001b[0m, in \u001b[0;36mMapRerankDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m, docs: List[Document], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    153\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map rerank manner.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reranking the results.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_and_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_results(docs, results)\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/llm.py:349\u001b[0m, in \u001b[0;36mLLMChain.apply_and_parse\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    344\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe apply_and_parse method is deprecated, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead pass an output parser directly to LLMChain.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(input_list, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/llm.py:355\u001b[0m, in \u001b[0;36mLLMChain._parse_generation\u001b[0;34m(self, generation)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_generation\u001b[39m(\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m, generation: List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[1;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse(res[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key])\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m generation\n\u001b[1;32m    358\u001b[0m         ]\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/llm.py:356\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_generation\u001b[39m(\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m, generation: List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]\n\u001b[1;32m    353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39moutput_parser \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 356\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m generation\n\u001b[1;32m    358\u001b[0m         ]\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/output_parsers/regex.py:35\u001b[0m, in \u001b[0;36mRegexParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_output_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     38\u001b[0m             key: text \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_output_key \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\n\u001b[1;32m     40\u001b[0m         }\n",
      "\u001b[0;31mValueError\u001b[0m: Could not parse output: Linear regression is a statistical model used to predict a real-valued output based on an input vector. It assumes that the regression function is linear or that the linear model is a reasonable approximation. The model consists of unknown parameters or coefficients (βj) and variables (Xj) that can come from different sources such as quantitative inputs, transformations of quantitative inputs, basis expansions, numeric or \"dummy\" coding of qualitative inputs, and interactions between variables. The parameters (β) are estimated using the least squares method, which minimizes the residual sum of squares."
     ]
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type=\"map_rerank\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    \"What is linear regression?\",\n",
    "    callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
