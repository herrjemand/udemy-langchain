{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The stuff strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\\nThis major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\\n›springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie • Tibshirani • Friedman\\nSecond Edition', metadata={'source': './mixed_data/ESLII_print12_toc (1).pdf', 'page': 0}),\n",
       " Document(page_content='This is page v\\nPrinter: Opaque this\\nTo our parents:\\nValerie and Patrick Hastie\\nVera and Sami Tibshirani\\nFlorence and Harry Friedman\\nand to our families:\\nSamantha, Timothy, and Lynda\\nCharlie, Ryan, Julie, and Cheryl\\nMelanie, Dora, Monika, and Ildiko', metadata={'source': './mixed_data/ESLII_print12_toc (1).pdf', 'page': 1}),\n",
       " Document(page_content='vi', metadata={'source': './mixed_data/ESLII_print12_toc (1).pdf', 'page': 2})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./mixed_data/ESLII_print12_toc (1).pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The stuff chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "index = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "100 3. Linear Methods for Regression\n",
      "\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression funct ion E(Y|X) is\n",
      "linear in the inputs X1,...,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s compu ter era there\n",
      "are still good reasons to study and use them. They are simple a nd often\n",
      "provide an adequate and interpretable description of how th e inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outp erform fancier\n",
      "nonlinear models, especially in situations with small numb ers of training\n",
      "cases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\n",
      "appliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\n",
      "scope. These generalizations are sometimes called basis-f unction methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, w hile in the\n",
      "nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an un derstanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of th e linear methods\n",
      "discussed here.\n",
      "\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "AsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression functio n E(Y|X) is\n",
      "linear, or that the linear model is a reasonable approximati on. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, squar e-root or\n",
      "square;\n",
      "•basisexpansions,suchas X2=X2\n",
      "1,X3=X3\n",
      "1,leadingtoapolynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative input s. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1·X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "methodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonab le if the training\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if theyi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, fo r example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was p roposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalt ies were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron e t al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits th e piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et al.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm simi lar to least\n",
      "angle regression for generalized regression models. Parti al least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage met hods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wide r? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\n",
      "parameteraTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that the relationship between the variables can be represented by a linear equation. The goal of linear regression is to find the best-fitting line that minimizes the difference between the observed data points and the predicted values from the linear equation. This line can then be used to make predictions or understand the relationship between the variables.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\n",
    "    \"What is linear regression?\",\n",
    "    callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Map Reduce strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "100 3. Linear Methods for Regression\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression funct ion E(Y|X) is\n",
      "linear in the inputs X1,...,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s compu ter era there\n",
      "are still good reasons to study and use them. They are simple a nd often\n",
      "provide an adequate and interpretable description of how th e inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outp erform fancier\n",
      "nonlinear models, especially in situations with small numb ers of training\n",
      "cases,lowsignal-to-noiseratioorsparsedata.Finally,l inearmethodscanbe\n",
      "appliedtotransformationsoftheinputsandthisconsidera blyexpandstheir\n",
      "scope. These generalizations are sometimes called basis-f unction methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, w hile in the\n",
      "nextchapter wediscusslinear methods for classiﬁcation. O nsometopics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an un derstanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of th e linear methods\n",
      "discussed here.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "AsintroducedinChapter2,wehaveaninputvector XT= (X1,X2,...,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression functio n E(Y|X) is\n",
      "linear, or that the linear model is a reasonable approximati on. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, squar e-root or\n",
      "square;\n",
      "•basisexpansions,suchas X2=X2\n",
      "1,X3=X3\n",
      "1,leadingtoapolynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative input s. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,...,5,such thatXj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1·X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Eachxi= (xi1,xi2,...,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "methodis least squares ,inwhichwepickthecoeﬃcients β= (β0,β1,...,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonab le if the training\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if theyi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, fo r example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regr ession was\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was p roposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalt ies were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron e t al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits th e piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its tra nsparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et al.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm simi lar to least\n",
      "angle regression for generalized regression models. Parti al least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage met hods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wide r? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estim ate of a\n",
      "parameteraTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds ifA−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Given the following extracted parts of a long document and a question, create a final answer. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "______________________\n",
      "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the variables, meaning that the change in the dependent variable is directly proportional to the change in the independent variables. The goal of linear regression is to find the best-fitting line that represents this relationship. The line is determined by estimating the coefficients of the independent variables, which represent the slope and intercept of the line. Linear regression is widely used in various fields for prediction and forecasting purposes.\n",
      "\n",
      "A linear regression model assumes that the regression function E(Y|X) is linear in the inputs X1,...,Xp. Linear models were largely developed in the precomputer age of statistics, but even in today’s computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs affect the output.\n",
      "\n",
      "Linear regression is a statistical modeling technique used to predict a real-valued output based on one or more input variables. It assumes a linear relationship between the input variables and the output variable. The goal is to find the best-fit line or hyperplane that minimizes the sum of the squared differences between the predicted values and the actual values of the output variable. The model is expressed as f(X) = β0 + ∑(Xjβj), where f(X) is the predicted output, β0 and βj are the unknown parameters or coefficients, and Xj represents the input variables.\n",
      "\n",
      "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the variables, and aims to find the best fitting line that represents this relationship. The line is determined by estimating the coefficients (or weights) of the independent variables, which can be used to predict the value of the dependent variable.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes that there is a linear relationship between the variables and aims to find the best-fitting line or hyperplane that represents this relationship. The model uses coefficients or weights to estimate the value of the dependent variable based on the values of the independent variables. It is widely used for prediction and forecasting purposes in various fields.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type=\"map_reduce\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    \"What is linear regression?\",\n",
    "    callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Refine strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unsupported chain type: . Should be one of dict_keys(['stuff', 'map_reduce', 'refine', 'map_rerank'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:105\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 105\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Github/Vistrem/learning/langchain-udemy/venv/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py:246\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m loader_mapping: Mapping[\u001b[38;5;28mstr\u001b[39m, LoadingCallable] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m: _load_stuff_chain,\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_reduce\u001b[39m\u001b[38;5;124m\"\u001b[39m: _load_map_reduce_chain,\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefine\u001b[39m\u001b[38;5;124m\"\u001b[39m: _load_refine_chain,\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_rerank\u001b[39m\u001b[38;5;124m\"\u001b[39m: _load_map_rerank_chain,\n\u001b[1;32m    244\u001b[0m }\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m     )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loader_mapping[chain_type](\n\u001b[1;32m    251\u001b[0m     llm, verbose\u001b[38;5;241m=\u001b[39mverbose, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    252\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Got unsupported chain type: . Should be one of dict_keys(['stuff', 'map_reduce', 'refine', 'map_rerank'])"
     ]
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type=\"refine\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    \"What is linear regression?\",\n",
    "    callbacks=[StdOutCallbackHandler()],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
